{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60085e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01edac9c-6584-4697-ba5c-47eca9b8f47a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a690e9-3b9f-4c9d-876c-ba52dd5ee35d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  2\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f927e8b0-5fe2-4e43-a873-67beb75736bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3082436e-9048-4525-ab33-161eb43fc8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "649cc23e-3cbb-42b2-a275-9c06762ea206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.18 (main, Sep 11 2023, 13:30:38) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a02b6ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f314d96",
   "metadata": {},
   "source": [
    "## HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f801e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d523c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade huggingface-hub\n",
    "# !pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a025c73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U huggingface_hub\n",
    "#!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eeee4b",
   "metadata": {},
   "source": [
    "## Step 3: Authenticate to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a806ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9828bc0abc34645a6625bc0bace51c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4badfba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af544f541bc41f8857f7795d46b389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a37dda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token can be pasted using 'Right-Click'.\n",
      "Token: ········\n",
      "Add token as git credential? (Y/n) Y\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\ly266e\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a5d1d2",
   "metadata": {},
   "source": [
    "## Step 4: Download the Llama 2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470f3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet bitsandbytes\n",
    "# !pip install --quiet transformers \n",
    "# !pip install --quiet accelerate\n",
    "# !pip install scipy numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e77a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4941d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3e9dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d618d9ed43084dc2a1e02e670347f14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)at-hf/resolve/main/tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ly266e\\Anaconda3\\envs\\GPU_Env\\lib\\site-packages\\huggingface_hub\\file_download.py:138: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ly266e\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22bdc235874719be735f49f42bf3d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b427207d86704de7a93c383dae0c5188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)2-7b-chat-hf/resolve/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665d376371f349d6942f9273487172c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-hf/resolve/main/special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f510ab2d6e4d18a95152c34b5f21d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ma-2-7b-chat-hf/resolve/main/config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a50047ba784853a722ae5ee3aed9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)esolve/main/model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae20ed28c83d4932957aea99232d64a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a2da15036a43d99b343a47998433d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "base_model_path=\"./huggingface/llama7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             device_map='auto',\n",
    "                                             torch_dtype=torch.float16)\n",
    "\n",
    "#Save the model and the tokenizer to your PC\n",
    "model.save_pretrained(base_model_path, from_pt=True) \n",
    "tokenizer.save_pretrained(base_model_path, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a8115",
   "metadata": {},
   "source": [
    "## Step 5: Load the Llama 2 model from the disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  LlamaForCausalLM, LlamaTokenizer, pipeline\n",
    "\n",
    "base_model_path=\"./huggingface/llama7B\"\n",
    "model = LlamaForCausalLM.from_pretrained(base_model_path)\n",
    "tokenizer =LlamaTokenizer.from_pretrained(base_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfd6e4",
   "metadata": {},
   "source": [
    "## Step6: Run interference using HuggingFace pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e39bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "sequences = pipe(\n",
    "    'I liked \"Maneskin\" and \"Pink Floyd\". Do you have any recommendations of other groups I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795405c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc251356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d717eb51",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HfApi' object has no attribute 'set_access_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# set api for login and save token\u001b[39;00m\n\u001b[0;32m     15\u001b[0m api\u001b[38;5;241m=\u001b[39mHfApi()\n\u001b[1;32m---> 16\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_access_token\u001b[49m(token)\n\u001b[0;32m     17\u001b[0m folder \u001b[38;5;241m=\u001b[39m HfFolder()\n\u001b[0;32m     18\u001b[0m folder\u001b[38;5;241m.\u001b[39msave_token(token)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HfApi' object has no attribute 'set_access_token'"
     ]
    }
   ],
   "source": [
    "# let us first install relavant libraries from HF\n",
    "# make sure that we are using the latest libraries which support logging-in via tokens\n",
    "# install or simple upgrade to latest version (upgrade needed on kaggle notebook)\n",
    "# for kaggle notebook, you may need to restart runtime to load the upgraded libraries correctly\n",
    "# !pip install --upgrade huggingface-hub\n",
    "# !pip install --upgrade transformers\n",
    "\n",
    "# get your account token from https://huggingface.co/settings/tokens\n",
    "token = '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'\n",
    "\n",
    "# import the relavant libraries for loggin in\n",
    "from huggingface_hub import HfApi, HfFolder, list_models\n",
    "\n",
    "# set api for login and save token\n",
    "api=HfApi()\n",
    "api.set_access_token(token)\n",
    "folder = HfFolder()\n",
    "folder.save_token(token)\n",
    "\n",
    "# # that's it! you are all good to go and continue your awesome work\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('myself/my_awesome_tokenizer', use_auth_token=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
